{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdf2text\n",
    "FILEPATH = \"/Users/vipul/Documents/211173_Resume.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import date\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# loading pre trained model\n",
    "model = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting name \n",
    "def get_name(resume):\n",
    "    rtext = model(resume)\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('NAME', [pattern])\n",
    "    matches = matcher(rtext)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = rtext[start:end]\n",
    "        return span.text\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phone number\n",
    "def get_mobno(resume):\n",
    "    pattern = r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\"\n",
    "    match = re.search(pattern, resume)\n",
    "    if match:\n",
    "        number = match.group()\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email\n",
    "def get_mail(resume):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", resume)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# education\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII', 'AISSCE', 'AISSE'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = model(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDUCATION = ['Education', 'Academic', 'Educational', 'Qualifications', 'Scholastic', 'Degree', 'Degrees', 'School']\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "DEGREE = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', 'B.Sc', 'BSc',\n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', 'M.Sc', 'MSc',\n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH',\n",
    "            'BA', 'B.A', 'MA', 'M.A',\n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII', 'AISSCE', 'AISSE'\n",
    "        ]\n",
    "\n",
    "UNI_KEYWORDS = ['school', 'university', 'college', 'institute', 'technology', 'school,', 'university,', 'college,', 'institute,', 'technology,']\n",
    "\n",
    "def get_education(data):\n",
    "    text_split = [i.strip() for i in data.split('\\n')]\n",
    "\n",
    "    # Finding education header\n",
    "    idx = 0\n",
    "    for i, x in enumerate(text_split):\n",
    "        for ptr in EDUCATION:\n",
    "            if re.search(ptr, x):\n",
    "                idx = i\n",
    "                break\n",
    "        if idx != 0: \n",
    "            break\n",
    "    txt = \"\\n\".join(text_split[idx:])\n",
    "\n",
    "    # Searching for degrees\n",
    "    nlp_text = model(txt)\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    deg = []\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in DEGREE and tex not in STOPWORDS:\n",
    "                deg.append(tex)\n",
    "\n",
    "    # Searching for year of joining\n",
    "    pattern = r'[0-9]{4}'\n",
    "    lst = re.findall(pattern, txt)\n",
    "    current_date = date.today()\n",
    "    current_year = current_date.year\n",
    "    yrs = []\n",
    "    for i in lst:\n",
    "        year = int(i)\n",
    "        if 1900 <= year <= (current_year + 10):\n",
    "            yrs.append(int(i))\n",
    "\n",
    "    # Searching for institute name\n",
    "    school_names = []\n",
    "    for phrase in text_split[idx:]:\n",
    "        p_key = set(phrase.lower().split(' ')) & set(UNI_KEYWORDS)\n",
    "        if (len(p_key) == 0 or phrase.lower() in UNI_KEYWORDS):\n",
    "            continue\n",
    "        school_names.append(phrase)\n",
    "\n",
    "    # Searching for percentage / GPA\n",
    "    pattern = r'((?:\\d{1,2}(?:\\.\\d{1,2})?|100)(?:\\s*%|\\/\\d+))'\n",
    "    gpa = re.findall(pattern, txt)\n",
    "\n",
    "\n",
    "    # Filtering out\n",
    "    l = min(len(deg), len(school_names))\n",
    "    edu = []\n",
    "    for i in range(l):\n",
    "        edu.append({\"degree\": deg[i], \"institute\": school_names[i], \"year\": 0, \"gpa\": \"\"})\n",
    "    for i in range(min(l, len(yrs))):\n",
    "        edu[i][\"year\"] = yrs[i]\n",
    "    for i in range(min(l, len(gpa))):\n",
    "        edu[i][\"gpa\"] = gpa[i]\n",
    "\n",
    "    return edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(resume_text):\n",
    "    nlp_text = model(resume_text)\n",
    "    noun_chunks = nlp_text.noun_chunks\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.skill_name.values)\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def get_links(FILEPATH):\n",
    "    PDFFile = open(FILEPATH,'rb')\n",
    "    PDF = PyPDF2.PdfReader(PDFFile)\n",
    "    pages = len(PDF.pages)\n",
    "    key = '/Annots'\n",
    "    uri = '/URI'\n",
    "    ank = '/A'\n",
    "\n",
    "    links = []\n",
    "    for page in range(pages):\n",
    "        print(\"Current Page: {}\".format(page))\n",
    "        pageSliced = PDF.pages[page]\n",
    "        pageObject = pageSliced.get_object()\n",
    "        if key in pageObject.keys():\n",
    "            ann = pageObject[key]\n",
    "            for a in ann:\n",
    "                u = a.get_object()\n",
    "                if uri in u[ank].keys():\n",
    "                    links.append(u[ank][uri])\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_linkedin(path):\n",
    "    p = re.compile('(http(s)?:\\/\\/)?([\\w]+\\.)?linkedin\\.com\\/(pub|in|profile)')\n",
    "    links = get_links(path)\n",
    "    for link in links:\n",
    "        if p.match(link):\n",
    "            return link\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def get_github(path):\n",
    "    p = re.compile('(http(s?):\\/\\/)?(www\\.)?github\\.([a-z])+\\/([A-Za-z0-9]{1,})+\\/?$')\n",
    "    links = get_links(path)\n",
    "    for link in links:\n",
    "        if p.match(link):\n",
    "            return link\n",
    "\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_resume(FILEPATH):\n",
    "    output_filename = os.path.basename(os.path.splitext(FILEPATH)[0]) + '.txt'\n",
    "    output_filepath = os.path.join('.', output_filename)\n",
    "    pdf2text.main(args=[FILEPATH, '--outfile', output_filepath])\n",
    "    data = open(output_filepath, \"r\").read()\n",
    "\n",
    "    name = get_name(data)\n",
    "    email = get_mail(data)\n",
    "    phno = get_mobno(data)\n",
    "    skills = extract_skills(data)\n",
    "    edu = get_education(data)\n",
    "    linkedin = get_linkedin(FILEPATH)\n",
    "    github = get_github(FILEPATH)\n",
    "\n",
    "    df = {\"name\": name, \"email\": email, \"mobile_no\": phno, \"linkedin-profile\": linkedin, \"github\": github, \"education\": edu, \"skills\": skills}\n",
    "    return json.dumps(df, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Page: 0\n",
      "Current Page: 0\n",
      "{\n",
      "    \"name\": \"Vipul Chanchlani\",\n",
      "    \"email\": \"vipulch21@iitk.ac.in\",\n",
      "    \"mobile_no\": \"+91-9462150839\",\n",
      "    \"linkedin-profile\": \"https://www.linkedin.com/in/vipul-chanchlani-61aa53227/\",\n",
      "    \"github\": \"https://github.com/vipul124\",\n",
      "    \"education\": [\n",
      "        {\n",
      "            \"degree\": \"BTech\",\n",
      "            \"institute\": \"Indian Institute of Technology Kanpur\",\n",
      "            \"year\": 2021,\n",
      "            \"gpa\": \"8.9/10\"\n",
      "        },\n",
      "        {\n",
      "            \"degree\": \"AISSCE\",\n",
      "            \"institute\": \"Sir Padampat Singhania School, Kota\",\n",
      "            \"year\": 2021,\n",
      "            \"gpa\": \"96 %\"\n",
      "        },\n",
      "        {\n",
      "            \"degree\": \"AISSE\",\n",
      "            \"institute\": \"Sir Padampat Singhania School, Kota\",\n",
      "            \"year\": 2019,\n",
      "            \"gpa\": \"94 %\"\n",
      "        }\n",
      "    ],\n",
      "    \"skills\": [\n",
      "        \"Selenium\",\n",
      "        \"C\",\n",
      "        \"C++\",\n",
      "        \"Python\",\n",
      "        \"Sqlite\",\n",
      "        \"Analytics\",\n",
      "        \"Research\",\n",
      "        \"Cryptography\",\n",
      "        \"Statistics\",\n",
      "        \"Electronics\",\n",
      "        \"Tensorflow\",\n",
      "        \"Git\",\n",
      "        \"Algorithms\",\n",
      "        \"Html\",\n",
      "        \"Chinese\",\n",
      "        \"Pandas\",\n",
      "        \"Wikipedia\",\n",
      "        \"Latex\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(parse_resume(FILEPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
